= OpenShift Virtualization

== Module Overview

**Duration:** 45-55 minutes (includes 10-15 min operator installation) +
**Format:** Hands-on Lab +
**Audience:** Infrastructure Architects, IT Operations

[IMPORTANT]
====
**Hardware Virtualization Requirements:**

OpenShift Virtualization requires hardware virtualization support (Intel VT-x/AMD-V with KVM).

**Supported platforms:**
- Bare metal servers with hardware virtualization enabled
- AWS metal instances (m5.metal, m5n.metal, etc.)
- Nested virtualization-enabled cloud instances

**This module will NOT work on:**
- Standard AWS EC2 instances (t3, m5, c5, etc.)
- Clusters without KVM support

If VM creation fails with `Insufficient devices.kubevirt.io/kvm`, your cluster doesn't support hardware virtualization.
====

== Learning Objectives

* Understand how VMs and containers run side-by-side on OpenShift
* Create and manage virtual machines using the Console and CLI
* Take VM snapshots for backup and rollback
* Monitor VM resource usage
* Set resource limits for VMs
* See the migration path from traditional VM infrastructure

== Why Virtualization on OpenShift?

**The Legacy Challenge:**
- Thousands of VMs in traditional infrastructure (VMware, RHV, HyperV)
- Can't containerize everything overnight  
- Operating two platforms is expensive

**The OpenShift Virt Solution:**
- Run VMs and Containers side-by-side
- Same management interface
- Gradual migration path

== Installing OpenShift Virtualization

First, let's check if OpenShift Virtualization is already installed:

[source,bash,role="execute"]
----
oc get csv -n openshift-cnv 2>/dev/null || echo "Not installed - proceeding with installation"
----

If not installed, follow these steps to install the operator:

=== Step 1: Create the Namespace

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-cnv
EOF
----

=== Step 2: Create the OperatorGroup

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: kubevirt-hyperconverged-group
  namespace: openshift-cnv
spec:
  targetNamespaces:
  - openshift-cnv
EOF
----

=== Step 3: Subscribe to the Operator

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: hco-operatorhub
  namespace: openshift-cnv
spec:
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  name: kubevirt-hyperconverged
  channel: "stable"
EOF
----

=== Step 4: Wait for the Operator to Install

This takes 2-3 minutes. Watch the installation progress:

[source,bash,role="execute"]
----
oc get csv -n openshift-cnv -w
----

Press `Ctrl+C` once you see `Succeeded` in the PHASE column.

Expected output:
----
NAME                                       DISPLAY                    VERSION   PHASE
kubevirt-hyperconverged-operator.v4.20.x   OpenShift Virtualization   4.20.x    Succeeded
----

NOTE: Version number will match your OpenShift cluster version.

=== Step 5: Create the HyperConverged Resource

This deploys all virtualization components:

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: openshift-cnv
spec: {}
EOF
----

=== Step 6: Wait for Components to Deploy

This takes 3-5 minutes. Watch the rollout:

[source,bash,role="execute"]
----
oc get pods -n openshift-cnv -w
----

Press `Ctrl+C` when all pods are Running.

You should see pods like:
----
virt-operator-xxxxx
virt-controller-xxxxx
virt-api-xxxxx
virt-handler-xxxxx (one per worker node)
----

[NOTE]
====
**Installation Requirements:**
- Cluster must support nested virtualization (bare metal or AWS metal instances)
- Worker nodes need sufficient CPU and memory
- If installation fails, check: `oc get hco -n openshift-cnv kubevirt-hyperconverged -o yaml`
====

== Verifying OpenShift Virtualization

Verify the operator is installed and ready:

[source,bash,role="execute"]
----
oc get csv -n openshift-cnv
----

Expected output:

----
NAME                                       DISPLAY                    VERSION
kubevirt-hyperconverged-operator.v4.20.3   OpenShift Virtualization   4.20.3
----

Check running pods:

[source,bash,role="execute"]
----
oc get pods -n openshift-cnv
----

You'll see virt-controller, virt-operator, and other virtualization components.

== Hands-On: Create Your First VM

=== Step 1: Using the Web Console

1. Navigate to **Virtualization → VirtualMachines**
2. Click **Create VirtualMachine**
3. Select **From Template**
4. Choose template:  
   - **Fedora** or **CentOS Stream**
   - Flavor: **Small** (1 CPU, 2Gi RAM)
5. Name: `ops-track-vm-01`
6. Click **Create**

=== Step 2: Start the VM

1. Click on your VM name
2. Click **Actions → Start**
3. Watch status: `Stopped` → `Starting` → `Running`

=== Step 3: Access via Console (VNC)

1. Click the **Console** tab
2. Select **VNC Console**
3. You now have graphical access inside your browser!

This is running a **full operating system** on OpenShift.

== CLI Management (Power Users)

Install `virtctl` CLI (if not already available):

[source,bash,role="execute"]
----
# Get the virtctl download URL from your cluster
VIRTCTL_URL=https://$(oc get route -n openshift-cnv hyperconverged-cluster-cli-download -o jsonpath='{.spec.host}')/amd64/linux/virtctl.tar.gz

# Download and install virtctl
curl -L $VIRTCTL_URL | tar -xzv
sudo mv virtctl /usr/local/bin/
virtctl version
----

[NOTE]
====
If you don't have sudo access, place virtctl in a directory in your PATH (e.g., `~/bin/`).
====

Manage VMs via CLI:

[source,bash,role="execute"]
----
# List VMs
oc get vms -A

# Get VM details (after creating a VM below)
# oc get vm ops-track-vm-01 -o yaml

# Check VM instances (running VMs)
oc get vmi -A
----

Stop and start VMs (after creating a VM):

[source,bash,role="copypaste"]
----
# Stop VM
virtctl stop ops-track-vm-01

# Start VM
virtctl start ops-track-vm-01

# Restart VM
virtctl restart ops-track-vm-01
----

== Unified Topology: VMs and Containers Together

1. Navigate to **Topology** view (Developer perspective)
2. You'll see:
   - **Containerized apps** (Pods, Deployments)
   - **Virtual machines** (VM resources)
   - All in the same view!

**This is the magic:** Everything managed in one platform.

== Live Migration

One of virtualization's killer features:

[source,bash,role="copypaste"]
----
# Migrate VM to another node (zero downtime)
virtctl migrate ops-track-vm-01
----

Watch the migration:

[source,bash,role="execute"]
----
oc get vmim
----

The VM moves to another worker node **without stopping**.

== Storage for VMs

VMs use PersistentVolumeClaims (same as containerized apps):

[source,bash,role="execute"]
----
# See VM disks
oc get pvc -l vm.kubevirt.io/name=ops-track-vm-01
----

Output:

----
NAME                      STATUS   VOLUME       CAPACITY   STORAGECLASS
ops-track-vm-01-disk-0    Bound    pvc-xxxxx    30Gi       gp3-csi
----

Same storage classes, same management (VMs use the same cloud-backed storage as containers).

== VM Snapshots (Backup and Rollback)

**Why snapshots matter for ops:** Take snapshots before upgrades, patches, or configuration changes. Quick rollback if something goes wrong.

=== Understanding VM Snapshots

VM snapshots capture the state of a VM at a specific point in time, including:
- VM configuration
- Disk contents (requires CSI-enabled storage)
- Memory state (optional)

Check snapshot API resources:

[source,bash,role="execute"]
----
oc api-resources | grep virtualmachine | grep snapshot
----

You'll see:
----
virtualmachinesnapshots                    vmsnapshot,vmsnapshots
virtualmachinesnapshotcontents             vmsnapshotcontent,vmsnapshotcontents
virtualmachinerestores                     vmrestore,vmrestores
----

=== Creating a VM Snapshot

[source,bash,role="copypaste"]
----
cat <<EOF | oc apply -f -
apiVersion: snapshot.kubevirt.io/v1beta1
kind: VirtualMachineSnapshot
metadata:
  name: ops-track-vm-01-snapshot
  namespace: default
spec:
  source:
    apiGroup: kubevirt.io
    kind: VirtualMachine
    name: ops-track-vm-01
EOF
----

Check snapshot status:

[source,bash,role="execute"]
----
oc get vmsnapshot -n default
----

[NOTE]
====
**Snapshot Requirements:**
- Storage class MUST support CSI volume snapshots
- VM can be running or stopped (online/offline snapshots)
- Offline snapshots (stopped VM) are more reliable
====

=== Restoring from a Snapshot

[source,bash,role="copypaste"]
----
cat <<EOF | oc apply -f -
apiVersion: snapshot.kubevirt.io/v1beta1
kind: VirtualMachineRestore
metadata:
  name: restore-ops-track-vm-01
  namespace: default
spec:
  target:
    apiGroup: kubevirt.io
    kind: VirtualMachine
    name: ops-track-vm-01
  virtualMachineSnapshotName: ops-track-vm-01-snapshot
EOF
----

Check restore status:

[source,bash,role="execute"]
----
oc get vmrestore -n default
----

[WARNING]
====
**Restoring overwrites the current VM state!** The VM must be stopped before restoring from a snapshot.
====

== VM Resource Management

**Why this matters for ops:** Prevent VMs from consuming all cluster resources. Set CPU/memory limits just like pods.

=== Understanding VM Resources

Check VM resource specification:

[source,bash,role="execute"]
----
oc explain vm.spec.template.spec.domain.resources
----

Output shows:
----
FIELDS:
  limits	<map[string]Object>
    Limits describes the maximum amount of compute resources allowed.
    Valid resource keys are "memory" and "cpu".

  requests	<map[string]Object>
    Requests describes the minimum amount of compute resources required.
----

=== Creating a VM with Resource Limits

[source,bash,role="copypaste"]
----
cat <<EOF | oc apply -f -
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: limited-vm
  namespace: default
spec:
  runStrategy: Manual
  template:
    metadata:
      labels:
        kubevirt.io/vm: limited-vm
    spec:
      domain:
        resources:
          requests:
            memory: 2Gi
            cpu: "1"
          limits:
            memory: 4Gi
            cpu: "2"
        devices:
          disks:
          - name: containerdisk
            disk:
              bus: virtio
      volumes:
      - name: containerdisk
        containerDisk:
          image: quay.io/containerdisks/fedora:40
EOF
----

This VM:
- **Requests**: 2Gi RAM, 1 CPU (guaranteed minimum)
- **Limits**: 4Gi RAM, 2 CPU (maximum allowed)

[TIP]
====
**Ops best practice:**
- Always set `requests` (scheduler needs this for placement)
- Set `limits` to prevent resource hogging
- Monitor actual usage and adjust over time
====

== Monitoring VMs

**Why this matters for ops:** Detect resource exhaustion, performance issues, and failing VMs before users complain.

=== Viewing VM Status

Basic VM status check:

[source,bash,role="execute"]
----
oc get vms -A
oc get vmi -A
----

Detailed VM information:

[source,bash,role="execute"]
----
# See CPU, memory, and conditions
oc describe vm ops-track-vm-01 -n default
oc describe vmi ops-track-vm-01 -n default
----

=== VM Metrics in the Web Console

Navigate to: **Virtualization → VirtualMachines → [Select VM] → Metrics**

You'll see:
- CPU usage over time
- Memory usage over time
- Network I/O
- Disk I/O
- Migration status

=== Checking VM Events

[source,bash,role="execute"]
----
# See VM lifecycle events
oc get events -n default --field-selector involvedObject.name=ops-track-vm-01
----

Look for:
- `Started` - VM was started
- `Stopped` - VM was stopped
- `FailedMigration` - Live migration failed
- `FailedScheduling` - Can't find node with resources

[NOTE]
====
**Common VM issues visible in events:**
- `Insufficient devices.kubevirt.io/kvm` - No hardware virt support
- `Insufficient cpu/memory` - Not enough resources on any node
- `FailedMount` - Storage (PVC) issues
====

== Migration Strategy

**The Hybrid Approach:**

1. **Keep existing VMs** - Migrate to OpenShift Virt
2. **Modernize over time** - Containerize when apps are updated
3. **Single platform** - Reduce operational burden

**Migration Tools:**
- **MTV** (Migration Toolkit for Virtualization)
- Import from VMware, RHV, OpenStack
- Automated workflows

== Verify Your VM is Working

Before moving to the next module, let's confirm everything is working correctly.

=== Final Verification Steps

**1. Check VM Status:**

[source,bash,role="execute"]
----
oc get vms -A
----

Expected output:

----
NAMESPACE   NAME              AGE    STATUS    READY
default     ops-track-vm-01   10m    Running   True
----

✅ **Success Criteria:** Status shows `Running` and Ready is `True`

**2. Check VM Instance (VMI):**

[source,bash,role="execute"]
----
oc get vmi
----

You should see:

----
NAME              AGE   PHASE     IP            NODENAME
ops-track-vm-01   10m   Running   10.xxx.x.xx   worker-1
----

✅ **Success Criteria:** Phase shows `Running` and VM has an IP address

**3. Verify Console Access:**

In the OpenShift web console:
1. Navigate to **Virtualization → VirtualMachines**
2. Click on `ops-track-vm-01`
3. Click the **Console** tab
4. You should see the VM's graphical console

✅ **Success Criteria:** You can see the VM's operating system in the browser

**4. Check VM Resources:**

[source,bash,role="execute"]
----
oc describe vm ops-track-vm-01
----

Look for:
- **Running**: True
- **Ready**: True
- **Created**: True

=== What You Accomplished

**Successfully Demonstrated:**

✅ OpenShift Virtualization operator is installed and working
✅ Created a virtual machine using the web console
✅ Started and accessed the VM (via VNC console)
✅ Managed VMs using CLI commands (`oc virt`)
✅ Verified VMs and containers appear in unified topology view
✅ Confirmed VM is running and accessible

**The VM is now running successfully alongside your containerized workloads!** ✅

This demonstrates the power of OpenShift Virtualization: running traditional VMs and modern containers on the same platform with the same management tools.

== Key Takeaways

✅ **VMs + Containers** on the same platform

✅ **Single pane of glass** management - use `oc` for resources, `virtctl` for VM operations

✅ **VM Snapshots** - backup before changes, quick rollback capability

✅ **Resource Management** - set CPU/memory limits to prevent resource exhaustion

✅ **Monitoring** - track VM metrics, events, and health status

✅ **Live Migration** - move VMs between nodes without downtime

✅ **Gradual migration** - migrate from VMware/RHV over time, not big-bang

✅ **Cost consolidation** - reduce VMware licenses, single platform overhead

✅ **Hardware Requirements** - needs KVM support (bare metal or metal instances)

== Next Module

Continue to **Multi-Cluster Management with ACM** to see how to manage multiple OpenShift clusters.

---

== Creating VMs Using the Modern UI Workflow

This section demonstrates the recommended approach for creating VMs using OpenShift Virtualization's InstanceTypes and pre-configured boot volumes.

=== Prerequisites: SSH Key Setup

Before creating VMs, we need to configure SSH access. An SSH key has been pre-configured for you.

**Generate and store the SSH key:**

From the bastion host, run:

[source,bash,role="execute"]
----
# Generate SSH key pair for VM access
ssh-keygen -t rsa -b 2048 -f /tmp/vm-ssh-key -N "" -C "workshop-vm-key"

# Create the secret in OpenShift
oc create secret generic bastion-ssh-key \
  --from-file=key=/tmp/vm-ssh-key.pub \
  -n default
----

Verify the secret was created:

[source,bash,role="execute"]
----
oc get secret bastion-ssh-key -n default
----

=== Step-by-Step: Creating a VM with InstanceTypes

==== 1. Navigate to Virtualization

In the OpenShift web console:

1. Click the **Virtualization** dropdown in the left navigation
2. Select **Virtualization** from the menu

image::Virt-01-showingdropdown.png[Virtualization dropdown menu]

==== 2. Start VM Creation

1. You'll see a "Welcome to OpenShift Virtualization" modal (first time only)
2. Click **Create VirtualMachine**

image::virt-02-welcometopenshift-createvirtualmachinebutton.png[Welcome screen with Create VirtualMachine button]

==== 3. Select Creation Method

Choose **InstanceTypes** tab (selected by default):

image::virt-03-createnewvirtualmachineoptions.png[Create new VirtualMachine options showing InstanceTypes]

==== 4. Select Boot Volume

**Step 1: Select volume to boot from**

Available boot volumes are pre-configured in the `openshift-virtualization-os-images` namespace:

- **centos-stream9** - CentOS Stream 9 (30 GiB)
- **centos-stream10** - CentOS Stream 10 (30 GiB)
- **fedora** - Fedora (amd64) (30 GiB)
- **rhel8** - Red Hat Enterprise Linux 8 (30 GiB)
- **rhel9** - Red Hat Enterprise Linux 9 (30 GiB)
- **rhel10** - Red Hat Enterprise Linux 10 (30 GiB)

For this workshop, select **rhel9** or **centos-stream9**.

image::virt-04-volumestoboot.png[List of available boot volumes]

All volumes use `gp3-csi` storage class and are 30 GiB in size.

==== 5. Select InstanceType

**Step 2: Select InstanceType**

InstanceTypes provide standardized VM sizing. Choose from Red Hat provided options:

- **General Purpose (U series)** - Balanced CPU/memory (recommended for workshop)
- **Compute Exclusive (CX series)** - CPU-intensive workloads
- **Memory Intensive (M series)** - Memory-heavy applications
- **Network (N series)** - Network-optimized VMs
- **Overcommitted (O series)** - Cost-optimized, shared resources
- **Realtime (RT series)** - Low-latency requirements

For this workshop, select **General Purpose (U series)**.

image::virt-05-selectinstanceboot.png[InstanceType selection showing six categories]

==== 6. Configure SSH Access

**Step 3: VirtualMachine details - Public SSH key**

1. Click on the **Public SSH key** field
2. Select **Use existing**
3. **Project**: Select `default`
4. **Public SSH key**: Select `bastion-ssh-key`
5. ✅ Check **"Automatically apply this key to any new VirtualMachine you create in this project"**
6. Click **Save**

image::virt-06-Public-SSH-key-details.png[SSH key configuration dialog]

==== 7. Finalize VM Details

**Step 3: VirtualMachine details**

Configure the following:

- **Name**: `workshop-vm-01`
- **Operating system**: Red Hat Enterprise Linux 9 (amd64) - _auto-filled_
- **InstanceType**: u1.medium - _auto-filled (1 CPU | 4 GiB Memory)_
- **Project**: `default`
- **Disk size**: `30` GiB
- **Storage class**: `gp3-csi`
- **Public SSH key**: `bastion-ssh-key` ✅
- **Dynamic SSH key injection**: _(toggle as needed)_

✅ Check **"Start this VirtualMachine after creation"**

image::virt-07-finaldetails.png[Final VM configuration details]

Click **Create VirtualMachine**.

=== Accessing Your VM

==== Monitor VM Startup

Watch the VM start in the console:

[source,bash,role="execute"]
----
# Watch VM status
oc get vms -w

# Press Ctrl+C when status shows Running
----

Expected output:
----
NAME              AGE   STATUS    READY
workshop-vm-01    30s   Running   True
----

==== Get VM IP Address

[source,bash,role="execute"]
----
# Get VM instance details including IP
oc get vmi workshop-vm-01 -o wide
----

Output will show:
----
NAME              AGE   PHASE     IP            NODENAME
workshop-vm-01    2m    Running   10.xxx.x.xx   worker-1
----

==== SSH to the VM

From the bastion host:

[source,bash,role="execute"]
----
# SSH using the private key we generated
ssh -i /tmp/vm-ssh-key cloud-user@<VM-IP>
----

Replace `<VM-IP>` with the IP address from the previous command.

[NOTE]
====
**Default usernames by OS:**
- RHEL/CentOS: `cloud-user`
- Fedora: `fedora`
- Ubuntu: `ubuntu`
====

=== Using the VNC Console

Alternative access method without SSH:

1. In the web console, navigate to **Virtualization → VirtualMachines**
2. Click on `workshop-vm-01`
3. Click the **Console** tab
4. Select **VNC Console**
5. You'll see the graphical console in your browser

=== What You Just Accomplished

✅ **Created a VM using InstanceTypes** - modern, declarative approach

✅ **Used pre-configured boot volumes** - faster than manual OS installation

✅ **Configured SSH access via secrets** - secure key management

✅ **Selected standardized VM sizing** - consistent resource allocation

✅ **Accessed VM via SSH and VNC** - multiple access methods

This workflow demonstrates OpenShift Virtualization's integration with Kubernetes patterns:
- Boot volumes as DataVolumes
- SSH keys as Secrets
- Standardized InstanceTypes
- Same storage classes as containers

---

**Questions?** Ask your instructor.
