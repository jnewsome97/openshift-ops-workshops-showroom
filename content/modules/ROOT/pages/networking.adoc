= Networking & Ingress

== Module Overview

**Duration:** 45 minutes +
**Format:** Hands-on networking configuration +
**Audience:** Platform Engineers, Network Administrators, Operations Teams

**Narrative Context:**

Your cluster is running applications. Now you need to understand:

- How traffic flows inside the cluster
- How external traffic reaches your applications
- How to secure network communication
- How to control egress traffic to external systems

== Learning Objectives

By the end of this module, you will be able to:

* Understand OpenShift's OVN-Kubernetes software-defined network
* Configure and troubleshoot the Ingress Controller
* Create Routes with different TLS termination strategies
* Implement NetworkPolicy for pod-to-pod security
* Configure EgressFirewall to control outbound traffic
* Use EgressIP for predictable source IPs
* Understand load balancing options

== OpenShift Networking Architecture

OpenShift 4.x uses **OVN-Kubernetes** as the default Container Network Interface (CNI) plugin. OVN-Kubernetes replaced the legacy OpenShift SDN in OpenShift 4.12.

**What OVN-Kubernetes provides:**

* Pod-to-pod networking across nodes
* Network isolation with NetworkPolicy
* Egress traffic control (EgressFirewall, EgressIP)
* IPsec encryption (optional)
* Hardware offloading support
* Integration with cloud provider load balancers

=== View Cluster Network Configuration

[source,bash,role="execute"]
----
oc get network.config cluster -o yaml
----

Key fields:

* `networkType: OVNKubernetes` - Confirms OVN-Kubernetes is active
* `clusterNetwork` - IP range for pod networking (typically 10.128.0.0/14)
* `serviceNetwork` - IP range for Services (typically 172.30.0.0/16)

=== Verify Network Operator Health

[source,bash,role="execute"]
----
oc get clusteroperator network
----

The network operator should show `AVAILABLE: True` and `DEGRADED: False`.

[source,bash,role="execute"]
----
oc get pods -n openshift-ovn-kubernetes
----

You'll see OVN pods running on every node: `ovnkube-node`, `ovnkube-controller`, and `ovs-daemon`.

== Ingress Controller & Load Balancing

The **Ingress Controller** (based on HAProxy) handles external traffic entering the cluster. It's how users reach your applications.

=== View Ingress Controller Configuration

[source,bash,role="execute"]
----
oc get ingresscontroller default -n openshift-ingress-operator -o yaml | head -60
----

Key configuration options:

* `replicas` - Number of router pods (default: 2 for HA)
* `routeAdmission.wildcardPolicy` - Whether wildcard routes are allowed
* `defaultCertificate` - TLS certificate for *.apps domain

=== View Router Pods

[source,bash,role="execute"]
----
oc get pods -n openshift-ingress -o wide
----

Router pods run on worker nodes and handle incoming HTTP/HTTPS traffic.

=== View Ingress Service

[source,bash,role="execute"]
----
oc get svc -n openshift-ingress
----

The `router-default` service exposes the ingress controller:

* On cloud platforms: `LoadBalancer` type with external IP
* On bare metal: Typically `NodePort` or MetalLB `LoadBalancer`

== Routes: OpenShift's Ingress Resource

OpenShift **Routes** expose services externally. They're more feature-rich than Kubernetes Ingress.

=== Use the Application from App Management

In the previous module, you deployed the `weathernow` application. Let's examine its networking.

[source,bash,role="execute"]
----
oc project app-management
----

[NOTE]
====
**Skipped the App Management module?**

Check if the weathernow app exists:

[source,bash,role="execute"]
----
oc get pods -n app-management -l deployment=weathernow
----

If no pods are found, deploy the app now:

[source,bash,role="execute"]
----
oc new-project app-management 2>/dev/null || oc project app-management
oc new-app --name=weathernow --image=quay.io/openshifttest/hello-openshift:1.2.0
----

Wait for the pod to be ready before continuing.
====

=== Create an Edge Route with TLS

First, let's create a route with edge TLS termination:

[source,bash,role="execute"]
----
oc create route edge weathernow --service=weathernow 2>/dev/null || echo "Route already exists"
----

=== View the Route Configuration

[source,bash,role="execute"]
----
oc get route weathernow -o yaml
----

Key route fields:

* `spec.host` - The external hostname
* `spec.to.name` - The backend Service
* `spec.tls.termination` - TLS termination type (edge in this case)

=== Test the Route

Test with HTTP first:

[source,bash,role="execute"]
----
curl -s http://$(oc get route weathernow -o jsonpath='{.spec.host}') | head -5
----

Test with HTTPS (edge TLS):

[source,bash,role="execute"]
----
curl -sk https://$(oc get route weathernow -o jsonpath='{.spec.host}') | head -5
----

This request flows: **Client → Load Balancer → Ingress Controller → Service → Pod**

You should see the WeatherNow HTML page returned.

=== TLS Termination Strategies

OpenShift Routes support three TLS termination types:

[cols="1,2,2"]
|===
|Type |Description |Use Case

|**edge**
|TLS terminates at the router, traffic to pod is HTTP
|Most common. Router handles certificates.

|**passthrough**
|TLS passes through to the pod unchanged
|Application manages its own certificates

|**reencrypt**
|TLS terminates at router, new TLS to pod
|End-to-end encryption with router certificate validation
|===

=== Examine TLS Configuration

View the TLS settings on the edge route:

[source,bash,role="execute"]
----
oc get route weathernow -o jsonpath='{.spec.tls}' | python3 -m json.tool
----

If the route has edge termination, you'll see:

[source,json]
----
{
    "termination": "edge"
}
----

Edge termination means:

* TLS terminates at the Ingress Controller (HAProxy)
* Traffic from router to pod is unencrypted HTTP
* The router uses the cluster's wildcard certificate

=== Create a Route with Custom TLS

For applications needing custom certificates, you can specify them:

[source,bash,role="copypaste"]
----
# Example: Create route with custom certificate (not executed)
oc create route edge myapp \
  --service=myapp \
  --cert=tls.crt \
  --key=tls.key \
  --ca-cert=ca.crt
----

=== Route Annotations for Advanced Features

Routes support annotations for rate limiting, timeouts, and more:

[source,bash,role="execute"]
----
# View available route annotations
oc explain route.metadata.annotations
----

Common annotations:

* `haproxy.router.openshift.io/timeout` - Backend timeout
* `haproxy.router.openshift.io/rate-limit-connections` - Connection rate limiting
* `haproxy.router.openshift.io/ip-whitelist` - IP-based access control

=== View Existing Routes Across Cluster

[source,bash,role="execute"]
----
oc get routes -A | head -20
----

Notice different termination types: `edge`, `passthrough`, `reencrypt`.

== NetworkPolicy: Pod-to-Pod Security

By default, all pods can communicate with all other pods. **NetworkPolicy** restricts this.

=== Create Test Projects

[source,bash,role="execute"]
----
# Create two projects for testing network isolation
oc new-project netpol-frontend
oc new-project netpol-backend

# Deploy apps in each
oc new-app --name=frontend --image=quay.io/openshifttest/hello-openshift:1.2.0 -n netpol-frontend
oc new-app --name=backend --image=quay.io/openshifttest/hello-openshift:1.2.0 -n netpol-backend
----

Wait for pods in both namespaces:

[source,bash,role="execute"]
----
oc get pods -n netpol-frontend
----

[source,bash,role="execute"]
----
oc get pods -n netpol-backend
----

Both pods should show `Running` before continuing.

=== Test Default Connectivity (Should Work)

Test connectivity from frontend to backend:

[source,bash,role="execute"]
----
BACKEND_IP=$(oc get pod -n netpol-backend -l deployment=backend -o jsonpath='{.items[0].status.podIP}')
oc exec -n netpol-frontend deployment/frontend -- curl -s --connect-timeout 5 http://${BACKEND_IP}:8080
----

Expected: `Hello OpenShift!` - connectivity works by default.

=== Apply Deny-All NetworkPolicy

[source,bash,role="execute"]
----
cat <<EOF | oc apply -n netpol-backend -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress: []
EOF
----

This blocks all incoming traffic to pods in `netpol-backend`.

=== Test Connectivity Again (Should Fail)

[source,bash,role="execute"]
----
BACKEND_IP=$(oc get pod -n netpol-backend -l deployment=backend -o jsonpath='{.items[0].status.podIP}')
oc exec -n netpol-frontend deployment/frontend -- curl -s --connect-timeout 5 http://${BACKEND_IP}:8080
----

Expected: Connection times out (no output after 5 seconds). The NetworkPolicy blocked it.

=== Allow Traffic from Specific Namespace

[source,bash,role="execute"]
----
# First, label the frontend namespace
oc label namespace netpol-frontend app-tier=frontend

# Create NetworkPolicy allowing traffic from frontend namespace
cat <<EOF | oc apply -n netpol-backend -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-frontend
spec:
  podSelector:
    matchLabels:
      deployment: backend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          app-tier: frontend
    ports:
    - protocol: TCP
      port: 8080
EOF
----

=== Test Connectivity (Should Work Again)

[source,bash,role="execute"]
----
BACKEND_IP=$(oc get pod -n netpol-backend -l deployment=backend -o jsonpath='{.items[0].status.podIP}')
oc exec -n netpol-frontend deployment/frontend -- curl -s --connect-timeout 5 http://${BACKEND_IP}:8080
----

Expected: `Hello OpenShift!` - the allow policy takes effect.

=== View NetworkPolicies

[source,bash,role="execute"]
----
oc get networkpolicy -n netpol-backend
----

You should see both `deny-all` and `allow-from-frontend` policies.

== EgressFirewall: Controlling Outbound Traffic

**EgressFirewall** controls which external IPs/domains pods can reach. This is an OVN-Kubernetes feature.

=== Understanding EgressFirewall

EgressFirewall applies per-namespace and evaluates rules in order. Traffic not matching any rule is **allowed** by default.

View the API:

[source,bash,role="execute"]
----
oc explain egressfirewall.spec
----

=== Create an EgressFirewall

Block traffic to a specific CIDR (example: block 1.1.1.0/24):

[source,bash,role="execute"]
----
cat <<EOF | oc apply -n netpol-frontend -f -
apiVersion: k8s.ovn.org/v1
kind: EgressFirewall
metadata:
  name: default
spec:
  egress:
  - type: Deny
    to:
      cidrSelector: 1.1.1.0/24
  - type: Allow
    to:
      cidrSelector: 0.0.0.0/0
EOF
----

This blocks egress to 1.1.1.0/24 but allows all other traffic.

=== Test EgressFirewall

[source,bash,role="execute"]
----
# This should fail (blocked)
oc exec -n netpol-frontend deployment/frontend -- curl -s --connect-timeout 3 http://1.1.1.1 || echo "Blocked!"

# This should work (allowed)
oc exec -n netpol-frontend deployment/frontend -- curl -s --connect-timeout 3 -I http://google.com | head -1
----

=== View EgressFirewall

[source,bash,role="execute"]
----
oc get egressfirewall -n netpol-frontend -o yaml
----

=== DNS-Based EgressFirewall

You can also use DNS names (OVN resolves them):

[source,bash,role="execute"]
----
cat <<EOF | oc apply -n netpol-backend -f -
apiVersion: k8s.ovn.org/v1
kind: EgressFirewall
metadata:
  name: default
spec:
  egress:
  - type: Allow
    to:
      dnsName: "*.redhat.com"
  - type: Allow
    to:
      dnsName: "quay.io"
  - type: Deny
    to:
      cidrSelector: 0.0.0.0/0
EOF
----

This allows only RedHat and Quay traffic, blocking everything else.

== EgressIP: Predictable Source IPs

**EgressIP** assigns a fixed source IP for egress traffic from specific pods. Useful when external firewalls need to allowlist your cluster.

=== Understanding EgressIP

[source,bash,role="execute"]
----
oc explain egressip
----

EgressIP requires:

1. Nodes labeled for hosting egress IPs
2. Available IPs on the node's network
3. EgressIP CR referencing namespace/pod selectors

=== View EgressIP Capability

[source,bash,role="execute"]
----
oc get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.metadata.annotations.k8s\.ovn\.org/node-egress-label}{"\n"}{end}'
----

Nodes need the annotation `k8s.ovn.org/node-egress-label` or the label `k8s.ovn.org/egress-assignable` to host EgressIPs.

**Note:** EgressIP configuration requires network planning and is typically done during cluster setup.

== Services: Internal Load Balancing

OpenShift Services provide internal load balancing and service discovery.

=== Service Types

[cols="1,2,2"]
|===
|Type |Description |Access

|**ClusterIP**
|Internal IP only
|Inside cluster only

|**NodePort**
|Port on every node
|External via node IP:port

|**LoadBalancer**
|Cloud/MetalLB external IP
|External via dedicated IP
|===

=== View Service Details

View the services in the app-management namespace:

[source,bash,role="execute"]
----
oc get svc -n app-management
----

=== View the Ingress LoadBalancer Service

The ingress controller itself uses a LoadBalancer service:

[source,bash,role="execute"]
----
oc get svc router-default -n openshift-ingress
----

On AWS, this is typically a Network Load Balancer (NLB). On bare metal, MetalLB can provide LoadBalancer functionality.

== DNS and Service Discovery

OpenShift provides automatic DNS for services.

=== DNS Format

Services are accessible at:

----
<service-name>.<namespace>.svc.cluster.local
----

=== Test DNS Resolution

Test DNS resolution from any pod. During the NetworkPolicy exercise, you can test:

[source,bash,role="execute"]
----
oc exec -n netpol-frontend deployment/frontend -- nslookup backend.netpol-backend.svc.cluster.local
----

Or test from a debug pod:

[source,bash,role="copypaste"]
----
oc debug -n app-management deployment/weathernow -- nslookup weathernow.app-management.svc.cluster.local
----

=== View Cluster DNS Configuration

[source,bash,role="execute"]
----
oc get dns cluster -o yaml
----

Shows:

* `baseDomain` - Cluster's base DNS domain
* `publicZone` - External DNS zone
* `privateZone` - Internal DNS zone

== Ingress Controller Scaling and Customization

=== Scale Ingress Controller

[source,bash,role="execute"]
----
oc patch ingresscontroller default -n openshift-ingress-operator --type=merge -p '{"spec":{"replicas":3}}'
----

Watch the new router pod come up:

[source,bash,role="execute"]
----
oc get pods -n openshift-ingress -w
----

Press Ctrl+C when you see 3 pods running.

=== View Ingress Controller Metrics

The router exposes metrics on port 1936, which are scraped by Prometheus. View them in the OpenShift console:

**Observe → Metrics** and query for `haproxy_` metrics.

Or check router status:

[source,bash,role="execute"]
----
oc get ingresscontroller default -n openshift-ingress-operator -o jsonpath='{.status.conditions}' | python3 -m json.tool
----

== Cleanup

[source,bash,role="execute"]
----
oc delete project netpol-frontend netpol-backend
----

== AWS WAF Integration

OpenShift on AWS can integrate with **AWS WAF** (Web Application Firewall) for advanced threat protection against OWASP Top 10 vulnerabilities including SQL injection, cross-site scripting (XSS), and other common attacks.

**Architecture:** AWS WAF attaches to an Application Load Balancer (ALB), which sits in front of your OpenShift workloads:

----
Client → AWS ALB + WAF → OpenShift Service → Pods
----

By default, OpenShift uses a Network Load Balancer (NLB) for ingress. To use WAF, you need the **AWS Load Balancer Operator** to create ALB-based ingress resources.

=== When to Use What

[cols="1,2,2"]
|===
|Scenario |Solution |Notes

|Basic rate limiting
|Route annotations
|Built-in, no extra cost

|IP-based blocking
|Route annotations or NetworkPolicy
|Built-in

|OWASP protection (SQLi, XSS)
|AWS WAF + ALB
|Requires ALB, extra cost

|DDoS protection
|AWS Shield + CloudFront
|Enterprise feature

|API security
|3scale API Management
|Full API gateway
|===

=== Built-in Protection (No AWS WAF)

Even without AWS WAF, OpenShift provides protection via route annotations:

[source,bash,role="execute"]
----
oc annotate route weathernow -n app-management \
  haproxy.router.openshift.io/rate-limit-connections=true \
  haproxy.router.openshift.io/rate-limit-connections.concurrent-tcp=50 \
  haproxy.router.openshift.io/rate-limit-connections.rate-http=100 \
  --overwrite
----

Verify the annotations were applied:

[source,bash,role="execute"]
----
oc get route weathernow -n app-management -o jsonpath='{.metadata.annotations}' | python3 -m json.tool | grep rate
----

'''

.**Optional: Hands-on AWS WAF Integration** (click to expand)
[%collapsible%closed]
====
**Duration:** 10-15 minutes +
**Prerequisites:** AWS CLI is pre-configured in your terminal

This exercise deploys a fully functional AWS WAF protecting an OpenShift application. You'll see WAF block SQL injection and XSS attacks in real-time.

[NOTE]
====
AWS CLI and credentials are pre-configured in your workshop terminal. Verify with:

[source,bash,role="execute"]
----
aws sts get-caller-identity
----

You should see your AWS account information.
====

==== Step 1: Install AWS Load Balancer Operator

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: aws-load-balancer-operator
  namespace: openshift-operators
spec:
  channel: stable-v1
  installPlanApproval: Automatic
  name: aws-load-balancer-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF
----

Wait for the operator to install:

[source,bash,role="execute"]
----
oc get csv -n openshift-operators -w | grep aws-load-balancer
----

Press Ctrl+C when you see `Succeeded`.

==== Step 2: Create AWS Load Balancer Controller

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: networking.olm.openshift.io/v1
kind: AWSLoadBalancerController
metadata:
  name: cluster
spec:
  subnetTagging: Auto
  additionalResourceTags:
    - key: Environment
      value: workshop
  ingressClass: alb
  config:
    replicas: 1
  enabledAddons:
    - AWSWAFv2
EOF
----

Wait for the controller pod:

[source,bash,role="execute"]
----
oc get pods -n openshift-operators -l app.kubernetes.io/name=aws-load-balancer-operator -w
----

Press Ctrl+C when you see `Running`.

==== Step 3: Create ALB-backed Ingress

Make sure the weathernow service is NodePort (required for ALB instance target type):

[source,bash,role="execute"]
----
oc patch svc weathernow -n app-management -p '{"spec":{"type":"NodePort"}}'
----

Create the Ingress:

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: weathernow-alb
  namespace: app-management
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: instance
spec:
  ingressClassName: alb
  rules:
  - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: weathernow
            port:
              number: 8080
EOF
----

Wait for the ALB to provision (2-3 minutes):

[source,bash,role="execute"]
----
oc get ingress weathernow-alb -n app-management -w
----

Press Ctrl+C when you see a hostname in the ADDRESS column.

==== Step 4: Create WAF WebACL

Create a WAF WebACL with AWS managed rule sets for SQL injection and common attacks:

[source,bash,role="execute"]
----
aws wafv2 create-web-acl \
  --name openshift-waf \
  --scope REGIONAL \
  --region us-east-2 \
  --default-action Allow={} \
  --visibility-config SampledRequestsEnabled=true,CloudWatchMetricsEnabled=true,MetricName=openshift-waf-metrics \
  --rules '[
    {
      "Name": "AWS-AWSManagedRulesCommonRuleSet",
      "Priority": 1,
      "Statement": {
        "ManagedRuleGroupStatement": {
          "VendorName": "AWS",
          "Name": "AWSManagedRulesCommonRuleSet"
        }
      },
      "OverrideAction": {"None": {}},
      "VisibilityConfig": {
        "SampledRequestsEnabled": true,
        "CloudWatchMetricsEnabled": true,
        "MetricName": "CommonRuleSet"
      }
    },
    {
      "Name": "AWS-AWSManagedRulesSQLiRuleSet",
      "Priority": 2,
      "Statement": {
        "ManagedRuleGroupStatement": {
          "VendorName": "AWS",
          "Name": "AWSManagedRulesSQLiRuleSet"
        }
      },
      "OverrideAction": {"None": {}},
      "VisibilityConfig": {
        "SampledRequestsEnabled": true,
        "CloudWatchMetricsEnabled": true,
        "MetricName": "SQLiRuleSet"
      }
    }
  ]'
----

==== Step 5: Associate WAF with ALB

Get the WAF ARN and annotate the Ingress:

[source,bash,role="execute"]
----
WAF_ARN=$(aws wafv2 list-web-acls --scope REGIONAL --region us-east-2 --query "WebACLs[?Name=='openshift-waf'].ARN" --output text)
echo "WAF ARN: $WAF_ARN"
oc annotate ingress weathernow-alb -n app-management alb.ingress.kubernetes.io/wafv2-acl-arn="$WAF_ARN" --overwrite
----

Verify the annotation:

[source,bash,role="execute"]
----
oc get ingress weathernow-alb -n app-management -o jsonpath='{.metadata.annotations.alb\.ingress\.kubernetes\.io/wafv2-acl-arn}'
echo
----

==== Step 6: Test WAF Protection

Get the ALB DNS name:

[source,bash,role="execute"]
----
ALB_DNS=$(oc get ingress weathernow-alb -n app-management -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo "ALB URL: http://$ALB_DNS"
----

Test normal request (should succeed):

[source,bash,role="execute"]
----
curl -s http://$ALB_DNS
----

Expected: `Hello OpenShift!`

Test SQL injection attack (should be blocked):

[source,bash,role="execute"]
----
curl -s "http://$ALB_DNS/?id=1%20OR%201=1"
----

Expected: `403 Forbidden`

Test XSS attack (should be blocked):

[source,bash,role="execute"]
----
curl -s "http://$ALB_DNS/?search=<script>alert(1)</script>"
----

Expected: `403 Forbidden`

**Congratulations!** You've successfully deployed AWS WAF protecting an OpenShift application. The WAF is blocking SQL injection and XSS attacks using AWS managed rule sets.

==== Cleanup (Optional)

To remove the WAF resources:

[source,bash,role="execute"]
----
# Delete the Ingress
oc delete ingress weathernow-alb -n app-management

# Delete the WAF WebACL (must have no associations first)
WAF_ID=$(aws wafv2 list-web-acls --scope REGIONAL --region us-east-2 --query "WebACLs[?Name=='openshift-waf'].Id" --output text)
LOCK_TOKEN=$(aws wafv2 list-web-acls --scope REGIONAL --region us-east-2 --query "WebACLs[?Name=='openshift-waf'].LockToken" --output text)
aws wafv2 delete-web-acl --name openshift-waf --scope REGIONAL --id $WAF_ID --lock-token $LOCK_TOKEN --region us-east-2
----
====

== Network Troubleshooting

=== Check Pod Connectivity

[source,bash,role="execute"]
----
# Get pod IPs
oc get pods -A -o wide | grep Running | head -5
----

=== Check Network Operator Logs

[source,bash,role="copypaste"]
----
oc logs -n openshift-network-operator deployment/network-operator --tail=50
----

=== Check OVN Controller Logs

[source,bash,role="copypaste"]
----
oc logs -n openshift-ovn-kubernetes ds/ovnkube-node -c ovnkube-controller --tail=30
----

=== Verify NetworkPolicy is Applied

[source,bash,role="copypaste"]
----
oc describe networkpolicy <policy-name> -n <namespace>
----

== Summary

**What you learned:**

* OpenShift uses OVN-Kubernetes as the default SDN
* The Ingress Controller (HAProxy) handles external traffic
* Routes expose services with TLS termination options
* NetworkPolicy controls pod-to-pod communication
* EgressFirewall controls outbound traffic to external destinations
* EgressIP provides predictable source IPs for egress
* Services provide internal load balancing and DNS discovery
* AWS WAF can be integrated via ALB for advanced threat protection
* Built-in rate limiting and IP whitelisting via route annotations

**Key operational commands:**

[source,bash]
----
# View network configuration
oc get network.config cluster -o yaml

# Check ingress controller
oc get ingresscontroller -n openshift-ingress-operator

# View routes
oc get routes -A

# View network policies
oc get networkpolicy -A

# View egress firewalls
oc get egressfirewall -A
----

== Additional Resources

* **OVN-Kubernetes:** link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/ovn-kubernetes_network_plugin/index[OVN-Kubernetes network plugin]
* **Ingress Controller:** link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/networking/configuring-ingress-cluster-traffic[Configuring ingress cluster traffic]
* **NetworkPolicy:** link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/networking/network-policy[About network policy]
* **EgressFirewall:** link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/networking/configuring-egress-firewall-ovn[Configuring an egress firewall for a project]
* **EgressIP:** link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/networking/configuring-egress-ips-ovn[Configuring an egress IP address]
* **AWS Load Balancer Operator:** link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/networking/aws-load-balancer-operator[AWS Load Balancer Operator]
* **Route Annotations:** link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/networking/configuring-ingress-cluster-traffic#nw-route-specific-annotations_route-configuration[Route-specific annotations]
