## OpenShift Monitoring
In this lab you will explore various aspects of the builtin OpenShift
Monitoring. This includes an overview of the OpenShift Alertmanager UI,
accessing the Prometheus web console, running PromQL (Prometheuses Query
Language) queries to inspect the cluster and finally looking at Grafana
dashboards.

### OpenShift Monitoring

OpenShift Container Platform includes a pre-configured, pre-installed, and
self-updating monitoring stack that is based on the Prometheus open source
project and its wider eco-system. It provides monitoring of cluster
components and includes a set of alerts to immediately notify the cluster
administrator about any occurring problems and a set of Grafana dashboards.
The cluster monitoring stack is only supported for monitoring OpenShift
Container Platform clusters.

#### Examine Alerting Configuration

1. Login to the link:{{ MASTER_URL }}[OpenShift Web Console] with the kubeadmin credentials. (Please do not use built-in console)
+
[source,role="copypaste"]
----
kubeadmin
----
+
[source,role="copypaste"]
----
{{ KUBEADMIN_PASSWORD }}
----
+
[Warning]
====
You may receive a self-signed certificate error in your browser when you
first visit the OpenShift Web console. When OpenShift is installed, by default, a CA
and SSL certificates are generated for all inter-component communication
within OpenShift, including the web console
====
+
1. On the left hand side, click on the "Observe" drop down.
1. Click on "Alerting". This is the OpenShift console's view of the alert configuration.

The Alerting tab shows you information about currently configured and/or
active alerts. You can see and do several things:

1. Filter alerts by their names.
1. Filter the alerts by their states. To fire, some alerts need a certain
  condition to be true for the duration of a timeout. If a condition of an
  alert is currently true, but the timeout has not been reached, such an alert
  is in the Pending state.
1. Alert name.
1. Description of an alert.
1. Current state of the alert and when the alert went into this state.
1. Value of the Severity label of the alert.
1. Actions you can do with the alert.

You'll also note that you can view alerts and associated notifications by clicking the bell icon at the top of the page.

##### Metrics UI (Prometheus Console)
In addition to the Alerting screen, OpenShift's built-in monitoring provides
an interface to access metrics collected by Prometheus using the link:https://prometheus.io/docs/prometheus/latest/querying/basics/[Prometheus
Query Language (PromQL)].

1. On the left hand side of the OpenShift Console, under the "Observe" section, click the link for "Metrics".

##### Running Prometheus Queries
Let's run a query to see the resources memory limit all pod definitions have defined.

1. Copy and paste the following query into the expression text box:
+
[source,role="copypaste"]
----
node_netstat_Udp6_OutDatagrams
----
+
1. Click the "Run Queries" button
1. You should now see a timeseries with a value in the list. You will also
   see a graph showing the value over the last time period (defaults to 30m).

Now let's run a query to see the cpu usage for the entire cluster.

1. Click "Add Query"
1. Copy and paste the following query into the query text box:
+
[source,role="copypaste"]
----
cluster:cpu_usage_cores:sum
----
+
1. Click the "Run Queries" button
1. You should now see a timeseries with a value in the list. This value is
  the latest gathered value for the timeseries as defined by this query.
  You'll also see that your new query got plotted on the same graph.

The metrics interface lets you run powerful queries on the information
collected about your cluster.

You'll also note that you can click directly into the Prometheus UI with a
link that's at the top of this page.

##### Dashboards UI (Grafana)
In addition to the Metrics UI, OpenShift monitoring provides a preconfigured
"Dashboards" UI (aka "Grafana"). The purpose of these Dashboards is to show
multiple metrics in easy to consume form graphed over time.

Click the "Dashboards" link under the "Observe" section on the left hand side.

.Dashboards OpenShift
image::images/ocp-grafana-replacement-new.png[]

Here you can click on the "Dashboard" arrow and select different metrics
to visualize. Go ahead and play around with this view. Note that this
is a visualization of the information that is provided by Prometheus.

### Key Takeaway

**Modern OpenShift Observability Stack:**

* **Prometheus** - Metrics (built-in) ✓
* **Grafana** - Visualization (built-in) ✓
* **Loki** - Log aggregation ✓
* **OpenTelemetry/Tempo** - Distributed tracing (optional, for advanced use cases)

You now have the foundation to monitor, visualize, and troubleshoot your
OpenShift clusters effectively!

.Advanced: OpenTelemetry and Distributed Tracing (Optional - 15-20 minutes)
[%collapsible]
====

**Duration:** 15-20 minutes +
**Level:** Advanced +
**Prerequisites:** Understanding of microservices architecture

While Prometheus provides excellent metrics collection and Grafana visualizes
them beautifully, modern cloud-native applications often need **distributed
tracing** to understand request flows across microservices.

==== What is OpenTelemetry?

**OpenTelemetry (OTel)** is an open-source observability framework that provides:

* **Traces** - Track requests as they flow through distributed systems
* **Metrics** - Similar to Prometheus, but with different collection methods
* **Logs** - Structured logging with context correlation

Think of observability as three pillars:

----
Metrics (Prometheus)  → What is happening? (CPU, memory, request rate)
Logs (Loki)          → What went wrong? (Error messages, events)
Traces (Tempo/OTel)  → Where is the bottleneck? (Request flow, latency)
----

==== When to Use OpenTelemetry

**Use Prometheus when:**
* You need cluster-wide metrics (CPU, memory, disk)
* You want to monitor infrastructure health
* You're tracking aggregated performance

**Use OpenTelemetry/Tempo when:**
* You have microservices calling each other
* You need to debug slow requests across services
* You want to see the complete request path (Service A → B → C → Database)

**Example Use Case:**

----
User reports: "The checkout page is slow"

Prometheus shows: API pod CPU is high (but why?)
Traces show:
  - Frontend → API: 50ms ✓
  - API → Payment Service: 2000ms ❌ (found the bottleneck!)
  - Payment Service → Database: 1900ms (root cause: slow query)
----

==== Hands-On: Installing Red Hat Tempo Operator

OpenShift 4.20+ supports OpenTelemetry through the **Red Hat build of Tempo**
operator, which provides distributed tracing capabilities.

===== Step 1: Verify Tempo Operator Availability

Check that the Tempo operator is available in your cluster:

[source,bash,role="execute"]
----
oc get packagemanifest tempo-product -n openshift-marketplace -o jsonpath='{.metadata.name}'
----

Expected output:
----
tempo-product
----

===== Step 2: Subscribe to Tempo Operator

The Tempo operator must be installed cluster-wide in the `openshift-operators` namespace:

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: tempo-operator
  namespace: openshift-operators
spec:
  channel: stable
  name: tempo-product
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF
----

Expected output:
----
subscription.operators.coreos.com/tempo-operator created
----

[NOTE]
====
**Why openshift-operators?** The Tempo operator only supports cluster-wide installation (`AllNamespaces` mode), so it must be installed in the `openshift-operators` namespace, not in a custom namespace.
====

===== Step 3: Verify Operator Installation

Wait 30-60 seconds for the operator to install, then check:

[source,bash,role="execute"]
----
oc get csv -n openshift-operators | grep tempo
----

Expected output (wait for PHASE: Succeeded):
----
tempo-operator.v0.19.0-2    Tempo Operator    0.19.0-2    tempo-operator.v0.19.0-1    Succeeded
----

[NOTE]
====
If the CSV doesn't appear immediately, wait another 30 seconds and try again. The operator installation takes time to complete.
====

===== Step 4: Check Operator Pods

Verify the Tempo operator pod is running:

[source,bash,role="execute"]
----
oc get pods -n openshift-operators | grep tempo
----

Expected output:
----
tempo-operator-controller-xxxxx-xxxxx    1/1     Running   0          2m
----

===== Step 5: Deploy Tempo Instance with Jaeger UI

Now let's deploy a TempoMonolithic instance to access the Jaeger UI for viewing traces:

[source,bash,role="execute"]
----
cat <<EOF | oc apply -f -
apiVersion: v1
kind: Namespace
metadata:
  name: tempo-demo
---
apiVersion: tempo.grafana.com/v1alpha1
kind: TempoMonolithic
metadata:
  name: tempo-sample
  namespace: tempo-demo
spec:
  storage:
    traces:
      backend: memory
      size: 1Gi
  resources:
    limits:
      cpu: "1"
      memory: 1Gi
  jaegerui:
    enabled: true
    route:
      enabled: true
EOF
----

Expected output:
----
namespace/tempo-demo created
tempomonolithic.tempo.grafana.com/tempo-sample created
----

Wait 60-90 seconds for the Tempo instance to deploy:

[source,bash,role="execute"]
----
oc get pods -n tempo-demo
----

Expected output (wait for 4/4 Running):
----
NAME                   READY   STATUS    RESTARTS   AGE
tempo-tempo-sample-0   4/4     Running   0          2m
----

===== Step 6: Access Jaeger UI

Get the Jaeger UI route URL:

[source,bash,role="execute"]
----
oc get route -n tempo-demo tempo-tempo-sample-jaegerui -o jsonpath='https://{.spec.host}{"\n"}'
----

This will output the URL to access the Jaeger UI, for example:
----
https://tempo-tempo-sample-jaegerui-tempo-demo.apps.cluster-xxxxx.com
----

[NOTE]
====
**What is Jaeger UI?** Jaeger is the industry-standard UI for viewing distributed traces. It allows you to:

* Search for traces by service, operation, or tags
* Visualize request flows across microservices
* Identify performance bottlenecks
* Analyze error patterns

While we've deployed the UI, actual trace data requires instrumenting applications with OpenTelemetry SDKs - covered in advanced workshops.
====

==== OpenTelemetry in Production

For production use, you would also:

1. **Create TempoStack Instance** - Deploy Tempo storage and query components
2. **Configure Object Storage** - Set up S3-compatible storage for traces
3. **Install OpenTelemetry Operator** - Collect and forward traces from applications
4. **Configure Instrumentation** - Auto-instrument applications to send traces
5. **Integrate with Applications** - Update apps to generate trace data

**For this 101 workshop:** Prometheus + Grafana + Loki provide comprehensive
observability for most operational needs. OpenTelemetry/Tempo is excellent for
deep microservices debugging and is covered in advanced observability workshops.

==== Cleanup (Optional)

If you want to remove the Tempo installation:

[source,bash,role="copypaste"]
----
# Delete the Tempo instance and demo namespace
oc delete namespace tempo-demo

# Delete the operator subscription and CSV
oc delete subscription.operators.coreos.com tempo-operator -n openshift-operators
oc delete csv tempo-operator.v0.19.0-2 -n openshift-operators
----

[NOTE]
====
We don't delete the `openshift-operators` namespace as it's used by other cluster-wide operators.
====

====
